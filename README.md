# gpu-llm-on-wsl
Dockerized environment for running quantized large language models with GPU acceleration on Windows via WSL2. Includes PyTorch, Hugging Face Transformers, and bitsandbytes for efficient local inference with Llamaâ€¯2, Mistral, and other 7B/13B models.
