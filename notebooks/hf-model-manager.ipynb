{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6db2dab",
   "metadata": {},
   "source": [
    "# Hugging Face Model Manager\n",
    "This notebook helps you securely manage your Hugging Face access token and download models or files from the Hub into this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086de635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import ipywidgets as widgets\n",
    "from huggingface_hub import HfApi, HfFolder, snapshot_download\n",
    "\n",
    "HF_CACHE = Path(os.getenv('HF_HOME', '/app/cache')).expanduser()\n",
    "HF_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "DEST_DIR = Path('/app/models')\n",
    "DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "def get_saved_token() -> Optional[str]:\n",
    "    token = HfFolder.get_token()\n",
    "    return token.strip() if token else None\n",
    "\n",
    "def save_token(token: str) -> None:\n",
    "    HfFolder.save_token(token.strip())\n",
    "\n",
    "display(widgets.HTML(f\"<b>Hugging Face cache:</b> {HF_CACHE}<br><b>Model destination:</b> {DEST_DIR}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e88e0",
   "metadata": {},
   "source": [
    "## 1. Manage your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c303d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_input = widgets.Password(\n",
    "    description='Token',\n",
    "    placeholder='hf_xxx',\n",
    "    layout=widgets.Layout(width='60%'),\n",
    "    value=get_saved_token() or ''\n",
    ")\n",
    "status_label = widgets.Label()\n",
    "save_button = widgets.Button(description='Save token', button_style='success', icon='check')\n",
    "clear_button = widgets.Button(description='Clear token', button_style='warning', icon='trash')\n",
    "\n",
    "def on_save_token(_):\n",
    "    if not token_input.value.strip():\n",
    "        status_label.value = '‚ö†Ô∏è Please enter a token before saving.'\n",
    "        return\n",
    "    save_token(token_input.value)\n",
    "    status_label.value = '‚úÖ Token saved to ~/.huggingface/token'\n",
    "\n",
    "def on_clear_token(_):\n",
    "    HfFolder.delete_token()\n",
    "    token_input.value = ''\n",
    "    status_label.value = 'üßπ Token cleared.'\n",
    "\n",
    "save_button.on_click(on_save_token)\n",
    "clear_button.on_click(on_clear_token)\n",
    "\n",
    "display(widgets.VBox([token_input, widgets.HBox([save_button, clear_button]), status_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02e4bd",
   "metadata": {},
   "source": [
    "## 2. Download models or files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_input = widgets.Text(\n",
    "    description='Repo ID',\n",
    "    placeholder='e.g. meta-llama/Llama-3.1-8B-Instruct',\n",
    "    layout=widgets.Layout(width='70%')\n",
    ")\n",
    "revision_input = widgets.Text(\n",
    "    description='Revision',\n",
    "    placeholder='main',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "pattern_input = widgets.Text(\n",
    "    description='File glob',\n",
    "    placeholder='*.bin (leave empty for all)',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "download_button = widgets.Button(description='Download', button_style='primary', icon='download')\n",
    "output_area = widgets.Output()\n",
    "progress = widgets.Label()\n",
    "\n",
    "def download_model(_):\n",
    "    output_area.clear_output()\n",
    "    token = token_input.value.strip() or get_saved_token()\n",
    "    if not repo_input.value.strip():\n",
    "        progress.value = '‚ö†Ô∏è Please provide a repository ID.'\n",
    "        return\n",
    "    if not token:\n",
    "        progress.value = '‚ö†Ô∏è An access token is required for most repos.'\n",
    "        return\n",
    "    progress.value = '‚è≥ Downloading... this may take a while.'\n",
    "    \n",
    "    # Create a subfolder based on the repo name (e.g., \"meta-llama/Llama-3.1-8B\" -> \"meta-llama--Llama-3.1-8B\")\n",
    "    repo_id = repo_input.value.strip()\n",
    "    safe_repo_name = repo_id.replace('/', '--')\n",
    "    model_dest = DEST_DIR / safe_repo_name\n",
    "    model_dest.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        local_path = snapshot_download(\n",
    "            repo_id=repo_id,\n",
    "            revision=revision_input.value.strip() or None,\n",
    "            cache_dir=str(HF_CACHE),\n",
    "            local_dir=str(model_dest),\n",
    "            local_dir_use_symlinks=False,\n",
    "            allow_patterns=pattern_input.value.strip() or None,\n",
    "            token=token\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        progress.value = f'‚ùå Download failed: {exc}'\n",
    "        return\n",
    "    progress.value = '‚úÖ Download complete.'\n",
    "    with output_area:\n",
    "        print(f'Model: {repo_id}')\n",
    "        print(f'Saved to: {local_path}')\n",
    "\n",
    "download_button.on_click(download_model)\n",
    "display(widgets.VBox([repo_input, revision_input, pattern_input, download_button, progress, output_area]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be98d2",
   "metadata": {},
   "source": [
    "## 3. View installed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import humanize\n",
    "\n",
    "def get_dir_size(path: Path) -> int:\n",
    "    \"\"\"Calculate total size of all files in a directory.\"\"\"\n",
    "    total = 0\n",
    "    try:\n",
    "        for item in path.rglob('*'):\n",
    "            if item.is_file():\n",
    "                total += item.stat().st_size\n",
    "    except (PermissionError, FileNotFoundError):\n",
    "        pass\n",
    "    return total\n",
    "\n",
    "def count_files(path: Path) -> int:\n",
    "    \"\"\"Count total files in a directory.\"\"\"\n",
    "    try:\n",
    "        return sum(1 for item in path.rglob('*') if item.is_file())\n",
    "    except (PermissionError, FileNotFoundError):\n",
    "        return 0\n",
    "\n",
    "def list_installed_models():\n",
    "    \"\"\"Display all models in the DEST_DIR.\"\"\"\n",
    "    models_output.clear_output()\n",
    "    \n",
    "    if not DEST_DIR.exists() or not any(DEST_DIR.iterdir()):\n",
    "        with models_output:\n",
    "            print('üì¶ No models found in /app/models/')\n",
    "        return\n",
    "    \n",
    "    model_dirs = [d for d in DEST_DIR.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not model_dirs:\n",
    "        with models_output:\n",
    "            print('üì¶ No models found in /app/models/')\n",
    "        return\n",
    "    \n",
    "    with models_output:\n",
    "        print(f'üì¶ Found {len(model_dirs)} model(s) in {DEST_DIR}:\\n')\n",
    "        print(f'{\"Model Name\":<50} {\"Files\":<10} {\"Size\":<15}')\n",
    "        print('-' * 75)\n",
    "        \n",
    "        for model_dir in sorted(model_dirs):\n",
    "            # Convert back from safe name (e.g., \"meta-llama--Llama-3.1-8B\" -> \"meta-llama/Llama-3.1-8B\")\n",
    "            display_name = model_dir.name.replace('--', '/', 1)\n",
    "            file_count = count_files(model_dir)\n",
    "            size_bytes = get_dir_size(model_dir)\n",
    "            size_human = humanize.naturalsize(size_bytes, binary=True) if size_bytes > 0 else '0 B'\n",
    "            \n",
    "            print(f'{display_name:<50} {file_count:<10} {size_human:<15}')\n",
    "\n",
    "models_output = widgets.Output()\n",
    "refresh_button = widgets.Button(description='Refresh', button_style='info', icon='refresh')\n",
    "\n",
    "def on_refresh(_):\n",
    "    list_installed_models()\n",
    "\n",
    "refresh_button.on_click(on_refresh)\n",
    "\n",
    "# Display initial list\n",
    "list_installed_models()\n",
    "\n",
    "display(widgets.VBox([refresh_button, models_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052bcf5",
   "metadata": {},
   "source": [
    "## 4. Load and test a model\n",
    "\n",
    "Use this section to load a downloaded model with GPU acceleration, quantization, and test inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d35276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import gc\n",
    "\n",
    "# Global model cache\n",
    "loaded_model = None\n",
    "loaded_tokenizer = None\n",
    "loaded_model_name = None\n",
    "\n",
    "def get_model_dirs():\n",
    "    \"\"\"Get list of downloaded models.\"\"\"\n",
    "    if not DEST_DIR.exists():\n",
    "        return []\n",
    "    return sorted([d for d in DEST_DIR.iterdir() if d.is_dir()])\n",
    "\n",
    "# Model selection dropdown\n",
    "model_dirs = get_model_dirs()\n",
    "model_choices = [('None', None)] + [(d.name.replace('--', '/', 1), str(d)) for d in model_dirs]\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_choices,\n",
    "    description='Model:',\n",
    "    layout=widgets.Layout(width='70%')\n",
    ")\n",
    "\n",
    "quantization_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('No quantization (full precision)', 'none'),\n",
    "        ('4-bit (NF4) - Recommended', '4bit'),\n",
    "        ('8-bit', '8bit')\n",
    "    ],\n",
    "    value='4bit',\n",
    "    description='Quantization:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(description='Load Model', button_style='primary', icon='upload')\n",
    "unload_button = widgets.Button(description='Unload Model', button_style='danger', icon='times')\n",
    "load_output = widgets.Output()\n",
    "load_status = widgets.Label()\n",
    "\n",
    "def load_model(_):\n",
    "    global loaded_model, loaded_tokenizer, loaded_model_name\n",
    "    \n",
    "    load_output.clear_output()\n",
    "    \n",
    "    if model_dropdown.value is None:\n",
    "        load_status.value = '‚ö†Ô∏è Please select a model.'\n",
    "        return\n",
    "    \n",
    "    model_path = model_dropdown.value\n",
    "    quant_mode = quantization_dropdown.value\n",
    "    \n",
    "    load_status.value = '‚è≥ Loading model... this may take a minute.'\n",
    "    \n",
    "    try:\n",
    "        with load_output:\n",
    "            print(f'üìÇ Model path: {model_path}')\n",
    "            print(f'‚öôÔ∏è  Quantization: {quant_mode}')\n",
    "            print(f'üîß Loading tokenizer...')\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        \n",
    "        with load_output:\n",
    "            print(f'‚úÖ Tokenizer loaded')\n",
    "            print(f'üîß Loading model...')\n",
    "        \n",
    "        # Configure quantization\n",
    "        if quant_mode == '4bit':\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                local_files_only=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        elif quant_mode == '8bit':\n",
    "            bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                local_files_only=True\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"auto\",\n",
    "                local_files_only=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        \n",
    "        loaded_model = model\n",
    "        loaded_tokenizer = tokenizer\n",
    "        loaded_model_name = model_path\n",
    "        \n",
    "        with load_output:\n",
    "            print(f'‚úÖ Model loaded successfully!')\n",
    "            print(f'üìä Device map: {model.hf_device_map if hasattr(model, \"hf_device_map\") else \"N/A\"}')\n",
    "            if torch.cuda.is_available():\n",
    "                print(f'üéÆ GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\n",
    "                print(f'üéÆ GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB')\n",
    "        \n",
    "        load_status.value = '‚úÖ Model loaded and ready for inference!'\n",
    "        \n",
    "    except Exception as exc:\n",
    "        load_status.value = f'‚ùå Load failed: {exc}'\n",
    "        with load_output:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def unload_model(_):\n",
    "    global loaded_model, loaded_tokenizer, loaded_model_name\n",
    "    \n",
    "    load_output.clear_output()\n",
    "    \n",
    "    if loaded_model is None:\n",
    "        load_status.value = '‚ö†Ô∏è No model is currently loaded.'\n",
    "        return\n",
    "    \n",
    "    with load_output:\n",
    "        print('üßπ Unloading model...')\n",
    "    \n",
    "    del loaded_model\n",
    "    del loaded_tokenizer\n",
    "    loaded_model = None\n",
    "    loaded_tokenizer = None\n",
    "    loaded_model_name = None\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    with load_output:\n",
    "        print('‚úÖ Model unloaded')\n",
    "        if torch.cuda.is_available():\n",
    "            print(f'üéÆ GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\n",
    "    \n",
    "    load_status.value = '‚úÖ Model unloaded, memory cleared.'\n",
    "\n",
    "load_button.on_click(load_model)\n",
    "unload_button.on_click(unload_model)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    model_dropdown,\n",
    "    quantization_dropdown,\n",
    "    widgets.HBox([load_button, unload_button]),\n",
    "    load_status,\n",
    "    load_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e7c45",
   "metadata": {},
   "source": [
    "## 5. Run inference\n",
    "\n",
    "Generate text with your loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7b71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = widgets.Textarea(\n",
    "    description='Prompt:',\n",
    "    placeholder='Enter your prompt here...',\n",
    "    layout=widgets.Layout(width='90%', height='100px')\n",
    ")\n",
    "\n",
    "max_tokens_slider = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=10,\n",
    "    max=2048,\n",
    "    step=10,\n",
    "    description='Max tokens:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "temperature_slider = widgets.FloatSlider(\n",
    "    value=0.7,\n",
    "    min=0.1,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description='Temperature:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "top_p_slider = widgets.FloatSlider(\n",
    "    value=0.9,\n",
    "    min=0.1,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='Top-p:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(description='Generate', button_style='success', icon='play')\n",
    "inference_output = widgets.Output()\n",
    "inference_status = widgets.Label()\n",
    "\n",
    "def generate_text(_):\n",
    "    global loaded_model, loaded_tokenizer\n",
    "    \n",
    "    inference_output.clear_output()\n",
    "    \n",
    "    if loaded_model is None or loaded_tokenizer is None:\n",
    "        inference_status.value = '‚ö†Ô∏è Please load a model first (see section 4).'\n",
    "        return\n",
    "    \n",
    "    if not prompt_input.value.strip():\n",
    "        inference_status.value = '‚ö†Ô∏è Please enter a prompt.'\n",
    "        return\n",
    "    \n",
    "    inference_status.value = '‚è≥ Generating...'\n",
    "    \n",
    "    try:\n",
    "        prompt = prompt_input.value.strip()\n",
    "        \n",
    "        with inference_output:\n",
    "            print(f'üéØ Prompt: {prompt}\\n')\n",
    "            print('-' * 80)\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = loaded_tokenizer(prompt, return_tensors=\"pt\").to(loaded_model.device)\n",
    "        \n",
    "        # Generate\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = loaded_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens_slider.value,\n",
    "                temperature=temperature_slider.value,\n",
    "                top_p=top_p_slider.value,\n",
    "                do_sample=True,\n",
    "                pad_token_id=loaded_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate tokens/sec\n",
    "        num_tokens = outputs.shape[1] - inputs.input_ids.shape[1]\n",
    "        tokens_per_sec = num_tokens / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        with inference_output:\n",
    "            print(f'üìù Generated text:\\n{generated_text}\\n')\n",
    "            print('-' * 80)\n",
    "            print(f'‚è±Ô∏è  Time: {elapsed:.2f}s | Tokens: {num_tokens} | Speed: {tokens_per_sec:.2f} tokens/s')\n",
    "        \n",
    "        inference_status.value = f'‚úÖ Generated {num_tokens} tokens in {elapsed:.2f}s'\n",
    "        \n",
    "    except Exception as exc:\n",
    "        inference_status.value = f'‚ùå Generation failed: {exc}'\n",
    "        with inference_output:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "generate_button.on_click(generate_text)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    prompt_input,\n",
    "    max_tokens_slider,\n",
    "    temperature_slider,\n",
    "    top_p_slider,\n",
    "    generate_button,\n",
    "    inference_status,\n",
    "    inference_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf4c43",
   "metadata": {},
   "source": [
    "## 6. System diagnostics\n",
    "\n",
    "Check GPU status, CUDA availability, and installed package versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "diag_button = widgets.Button(description='Run Diagnostics', button_style='info', icon='stethoscope')\n",
    "diag_output = widgets.Output()\n",
    "\n",
    "def run_diagnostics(_):\n",
    "    diag_output.clear_output()\n",
    "    \n",
    "    with diag_output:\n",
    "        print('=' * 80)\n",
    "        print('GPU LLM Environment - System Diagnostics')\n",
    "        print('=' * 80)\n",
    "        print()\n",
    "        \n",
    "        # Python & Core Libraries\n",
    "        print('üì¶ CORE PACKAGES')\n",
    "        print(f'  PyTorch version: {torch.__version__}')\n",
    "        print(f'  Transformers version: {transformers.__version__}')\n",
    "        print(f'  Accelerate version: {accelerate.__version__}')\n",
    "        print(f'  Bitsandbytes version: {bitsandbytes.__version__}')\n",
    "        print()\n",
    "        \n",
    "        # CUDA & GPU\n",
    "        print('üéÆ CUDA & GPU')\n",
    "        print(f'  CUDA available: {torch.cuda.is_available()}')\n",
    "        if torch.cuda.is_available():\n",
    "            print(f'  CUDA version: {torch.version.cuda}')\n",
    "            print(f'  cuDNN version: {torch.backends.cudnn.version()}')\n",
    "            print(f'  Number of GPUs: {torch.cuda.device_count()}')\n",
    "            print()\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                print(f'    Total memory: {props.total_memory / 1024**3:.2f} GB')\n",
    "                print(f'    Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB')\n",
    "                print(f'    Reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB')\n",
    "                print(f'    Free: {(props.total_memory - torch.cuda.memory_reserved(i)) / 1024**3:.2f} GB')\n",
    "        else:\n",
    "            print('  ‚ö†Ô∏è  No GPU detected!')\n",
    "        print()\n",
    "        \n",
    "        # Bitsandbytes check\n",
    "        print('üîß BITSANDBYTES')\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            print(f'  Status: ‚úÖ Available')\n",
    "            print(f'  CUDA support: {bnb.cuda_setup.common.get_cuda_lib_handle() is not None}')\n",
    "        except Exception as e:\n",
    "            print(f'  Status: ‚ùå Error: {e}')\n",
    "        print()\n",
    "        \n",
    "        # Model status\n",
    "        print('ü§ñ LOADED MODEL')\n",
    "        if loaded_model is not None:\n",
    "            print(f'  Model: {loaded_model_name}')\n",
    "            print(f'  Device: {loaded_model.device if hasattr(loaded_model, \"device\") else \"N/A\"}')\n",
    "            print(f'  Device map: {loaded_model.hf_device_map if hasattr(loaded_model, \"hf_device_map\") else \"N/A\"}')\n",
    "        else:\n",
    "            print('  No model currently loaded')\n",
    "        print()\n",
    "        \n",
    "        print('=' * 80)\n",
    "\n",
    "diag_button.on_click(run_diagnostics)\n",
    "\n",
    "display(widgets.VBox([diag_button, diag_output]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
